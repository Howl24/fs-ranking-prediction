{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "# pd.set_option('display.max_columns', 1000)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "sns.set()\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('meta_features.csv')\n",
    "ranking  = pd.read_csv('ranking_fs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>X_correlation_max</th>\n",
       "      <th>X_correlation_mean</th>\n",
       "      <th>X_correlation_min</th>\n",
       "      <th>X_covariance_max</th>\n",
       "      <th>X_covariance_mean</th>\n",
       "      <th>X_covariance_min</th>\n",
       "      <th>X_exp_var_max</th>\n",
       "      <th>X_exp_var_n_t80_cumsum</th>\n",
       "      <th>X_kurtosis_max</th>\n",
       "      <th>...</th>\n",
       "      <th>X_stand_dev_min</th>\n",
       "      <th>X_std_covariance_max</th>\n",
       "      <th>X_std_covariance_mean</th>\n",
       "      <th>X_std_covariance_min</th>\n",
       "      <th>X_std_exp_var_max</th>\n",
       "      <th>X_var_coef_max</th>\n",
       "      <th>X_var_coef_mean</th>\n",
       "      <th>X_var_coef_min</th>\n",
       "      <th>y_norm_class_entropy_none</th>\n",
       "      <th>y_num_classes_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_prostate_singh</td>\n",
       "      <td>0.994905</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>-0.989840</td>\n",
       "      <td>6.501921e+06</td>\n",
       "      <td>29.654376</td>\n",
       "      <td>-1.662059e+06</td>\n",
       "      <td>0.538233</td>\n",
       "      <td>3</td>\n",
       "      <td>96.809515</td>\n",
       "      <td>...</td>\n",
       "      <td>45.069954</td>\n",
       "      <td>0.994905</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>-0.989840</td>\n",
       "      <td>0.635789</td>\n",
       "      <td>24.644940</td>\n",
       "      <td>0.968532</td>\n",
       "      <td>0.393882</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_glioma_phillips</td>\n",
       "      <td>0.993589</td>\n",
       "      <td>0.026853</td>\n",
       "      <td>-0.690733</td>\n",
       "      <td>4.323435e+08</td>\n",
       "      <td>245013.502247</td>\n",
       "      <td>-6.592495e+07</td>\n",
       "      <td>0.311808</td>\n",
       "      <td>16</td>\n",
       "      <td>94.998000</td>\n",
       "      <td>...</td>\n",
       "      <td>849.046524</td>\n",
       "      <td>0.993589</td>\n",
       "      <td>0.026853</td>\n",
       "      <td>-0.690733</td>\n",
       "      <td>0.144017</td>\n",
       "      <td>6.143025</td>\n",
       "      <td>0.497883</td>\n",
       "      <td>0.196919</td>\n",
       "      <td>0.795040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_leukemia_armstrong</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.084879</td>\n",
       "      <td>-0.916524</td>\n",
       "      <td>1.154806e+08</td>\n",
       "      <td>556524.637475</td>\n",
       "      <td>-7.348629e+07</td>\n",
       "      <td>0.249095</td>\n",
       "      <td>19</td>\n",
       "      <td>64.783987</td>\n",
       "      <td>...</td>\n",
       "      <td>1480.763026</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.084879</td>\n",
       "      <td>-0.916524</td>\n",
       "      <td>0.241996</td>\n",
       "      <td>2.250262</td>\n",
       "      <td>0.511498</td>\n",
       "      <td>0.239257</td>\n",
       "      <td>0.991532</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_breast_sotiriou</td>\n",
       "      <td>0.997310</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>-0.803858</td>\n",
       "      <td>6.244864e+00</td>\n",
       "      <td>1.451080</td>\n",
       "      <td>-5.063628e+00</td>\n",
       "      <td>0.721969</td>\n",
       "      <td>3</td>\n",
       "      <td>10.362369</td>\n",
       "      <td>...</td>\n",
       "      <td>1.308462</td>\n",
       "      <td>0.997310</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>-0.803858</td>\n",
       "      <td>0.745195</td>\n",
       "      <td>0.389483</td>\n",
       "      <td>0.202296</td>\n",
       "      <td>0.182322</td>\n",
       "      <td>0.993717</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_lymphoma_dave_2</td>\n",
       "      <td>0.985847</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>-0.739594</td>\n",
       "      <td>5.893252e+00</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>-3.396945e+00</td>\n",
       "      <td>0.124228</td>\n",
       "      <td>93</td>\n",
       "      <td>35.139636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610939</td>\n",
       "      <td>0.985847</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>-0.739594</td>\n",
       "      <td>0.140919</td>\n",
       "      <td>0.525447</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.882052</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      dataset  X_correlation_max  X_correlation_mean  \\\n",
       "0      dataset_prostate_singh           0.994905            0.001368   \n",
       "1     dataset_glioma_phillips           0.993589            0.026853   \n",
       "2  dataset_leukemia_armstrong           0.994280            0.084879   \n",
       "3     dataset_breast_sotiriou           0.997310            0.703431   \n",
       "4     dataset_lymphoma_dave_2           0.985847            0.019387   \n",
       "\n",
       "   X_correlation_min  X_covariance_max  X_covariance_mean  X_covariance_min  \\\n",
       "0          -0.989840      6.501921e+06          29.654376     -1.662059e+06   \n",
       "1          -0.690733      4.323435e+08      245013.502247     -6.592495e+07   \n",
       "2          -0.916524      1.154806e+08      556524.637475     -7.348629e+07   \n",
       "3          -0.803858      6.244864e+00           1.451080     -5.063628e+00   \n",
       "4          -0.739594      5.893252e+00           0.027503     -3.396945e+00   \n",
       "\n",
       "   X_exp_var_max  X_exp_var_n_t80_cumsum  X_kurtosis_max         ...          \\\n",
       "0       0.538233                       3       96.809515         ...           \n",
       "1       0.311808                      16       94.998000         ...           \n",
       "2       0.249095                      19       64.783987         ...           \n",
       "3       0.721969                       3       10.362369         ...           \n",
       "4       0.124228                      93       35.139636         ...           \n",
       "\n",
       "   X_stand_dev_min  X_std_covariance_max  X_std_covariance_mean  \\\n",
       "0        45.069954              0.994905               0.001368   \n",
       "1       849.046524              0.993589               0.026853   \n",
       "2      1480.763026              0.994280               0.084879   \n",
       "3         1.308462              0.997310               0.703431   \n",
       "4         0.610939              0.985847               0.019387   \n",
       "\n",
       "   X_std_covariance_min  X_std_exp_var_max  X_var_coef_max  X_var_coef_mean  \\\n",
       "0             -0.989840           0.635789       24.644940         0.968532   \n",
       "1             -0.690733           0.144017        6.143025         0.497883   \n",
       "2             -0.916524           0.241996        2.250262         0.511498   \n",
       "3             -0.803858           0.745195        0.389483         0.202296   \n",
       "4             -0.739594           0.140919        0.525447         0.158073   \n",
       "\n",
       "   X_var_coef_min  y_norm_class_entropy_none  y_num_classes_none  \n",
       "0        0.393882                   0.999723                   2  \n",
       "1        0.196919                   0.795040                   2  \n",
       "2        0.239257                   0.991532                   3  \n",
       "3        0.182322                   0.993717                   3  \n",
       "4        0.086560                   0.882052                   4  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>minmax_chi_square_naiveBayes</th>\n",
       "      <th>minmax_fisher_naiveBayes</th>\n",
       "      <th>minmax_reliefF_naiveBayes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_prostate_singh</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_glioma_phillips</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_leukemia_armstrong</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_breast_sotiriou</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_lymphoma_dave_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      dataset  minmax_chi_square_naiveBayes  \\\n",
       "0      dataset_prostate_singh                             2   \n",
       "1     dataset_glioma_phillips                             3   \n",
       "2  dataset_leukemia_armstrong                             2   \n",
       "3     dataset_breast_sotiriou                             3   \n",
       "4     dataset_lymphoma_dave_2                             2   \n",
       "\n",
       "   minmax_fisher_naiveBayes  minmax_reliefF_naiveBayes  \n",
       "0                         3                          1  \n",
       "1                         2                          1  \n",
       "2                         3                          1  \n",
       "3                         1                          2  \n",
       "4                         1                          3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 40), (60, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankerNet(nn.Module):\n",
    "    def __init__(self, dataset_sz, ranker_sz, latent_sz):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dataset_sz, latent_sz)\n",
    "        self.embedding = nn.Embedding(ranker_sz, latent_sz)\n",
    "#         self.linear.weight.data.uniform_(0, 0.05)\n",
    "#         self.embedding.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, dataset_features, ranker_index):\n",
    "        latent_dataset = self.linear(dataset_features)\n",
    "        latent_ranker  = self.embedding(ranker_index)\n",
    "        output = (latent_dataset * latent_ranker).sum(1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankerNet(39, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset = torch.FloatTensor(features.iloc[:2, 1:].values)\n",
    "x_ranker  = torch.LongTensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1817e+07,  3.6183e+09])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_dataset, x_ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = features.iloc[:, 1:].values.repeat(3, axis=0)\n",
    "ranker  = np.concatenate([np.zeros(len(ranking)),\n",
    "                          np.ones(len(ranking)),\n",
    "                          np.ones(len(ranking)) + 1]).astype(int)\n",
    "target  = np.concatenate([ranking.minmax_chi_square_naiveBayes.values,\n",
    "                          ranking.minmax_fisher_naiveBayes,\n",
    "                          ranking.minmax_reliefF_naiveBayes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180, 39), (180,), (180,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape, ranker.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankerDataset(Dataset):\n",
    "    def __init__(self, dataset, ranker, target):\n",
    "        self.dataset = dataset.astype(np.float32)\n",
    "        self.ranker = ranker.astype(np.int64)\n",
    "        self.target = target.astype(np.float32) \\\n",
    "                        if target is not None else \\\n",
    "                        np.zeros(len(dataset)).astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.dataset[idx], self.ranker[idx], self.target[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RankerDataset(dataset, ranker, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataset, ranker, target, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(dataset, ranker)\n",
    "    loss = criterion(preds.view(-1), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion,\n",
    "                n_epochs, print_every=1, USE_CUDA=False):\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (dataset, ranker, target) in enumerate(train_loader):\n",
    "            train_loss += train_step(model, dataset, ranker, target, optimizer, criterion)\n",
    "            if batch_idx > 0 and batch_idx % print_every == 0:\n",
    "                train_loss /= print_every\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch + 1, batch_idx * len(dataset), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), train_loss))\n",
    "                train_losses.append(train_loss)\n",
    "                train_loss = 0\n",
    "        \n",
    "        print()\n",
    "    return model, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.992072</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>-0.784183</td>\n",
       "      <td>2.039197e+09</td>\n",
       "      <td>5.552481e+05</td>\n",
       "      <td>-4.183488e+08</td>\n",
       "      <td>0.307231</td>\n",
       "      <td>20.433333</td>\n",
       "      <td>67.791047</td>\n",
       "      <td>5.315244</td>\n",
       "      <td>...</td>\n",
       "      <td>408.621286</td>\n",
       "      <td>0.992072</td>\n",
       "      <td>0.091662</td>\n",
       "      <td>-0.784183</td>\n",
       "      <td>0.218465</td>\n",
       "      <td>9.269826</td>\n",
       "      <td>-12.284393</td>\n",
       "      <td>-44.985478</td>\n",
       "      <td>0.923358</td>\n",
       "      <td>2.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.173092</td>\n",
       "      <td>0.163485</td>\n",
       "      <td>1.092835e+10</td>\n",
       "      <td>1.872173e+06</td>\n",
       "      <td>2.286736e+09</td>\n",
       "      <td>0.198924</td>\n",
       "      <td>19.677313</td>\n",
       "      <td>52.806781</td>\n",
       "      <td>4.324071</td>\n",
       "      <td>...</td>\n",
       "      <td>699.357013</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.173092</td>\n",
       "      <td>0.163485</td>\n",
       "      <td>0.164280</td>\n",
       "      <td>58.894120</td>\n",
       "      <td>85.276180</td>\n",
       "      <td>289.088442</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>0.887373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.915535</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.996293</td>\n",
       "      <td>6.122095e-02</td>\n",
       "      <td>-2.411734e+03</td>\n",
       "      <td>-1.765460e+10</td>\n",
       "      <td>0.034849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.362369</td>\n",
       "      <td>-0.156562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041068</td>\n",
       "      <td>0.915535</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.996293</td>\n",
       "      <td>0.033770</td>\n",
       "      <td>-254.020889</td>\n",
       "      <td>-658.725753</td>\n",
       "      <td>-2243.745500</td>\n",
       "      <td>0.629249</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.993146</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>-0.889751</td>\n",
       "      <td>6.712167e+00</td>\n",
       "      <td>3.004735e-02</td>\n",
       "      <td>-6.626037e+07</td>\n",
       "      <td>0.176287</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>34.342161</td>\n",
       "      <td>1.674326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758121</td>\n",
       "      <td>0.993146</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>-0.889751</td>\n",
       "      <td>0.130432</td>\n",
       "      <td>0.518238</td>\n",
       "      <td>0.193711</td>\n",
       "      <td>0.066518</td>\n",
       "      <td>0.909347</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.996141</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>-0.808215</td>\n",
       "      <td>7.046733e+06</td>\n",
       "      <td>2.102329e+02</td>\n",
       "      <td>-1.388524e+06</td>\n",
       "      <td>0.246706</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>49.015076</td>\n",
       "      <td>4.068749</td>\n",
       "      <td>...</td>\n",
       "      <td>52.980775</td>\n",
       "      <td>0.996141</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>-0.808215</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>5.833418</td>\n",
       "      <td>0.497587</td>\n",
       "      <td>0.153729</td>\n",
       "      <td>0.957392</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.078478</td>\n",
       "      <td>-0.714148</td>\n",
       "      <td>2.342942e+08</td>\n",
       "      <td>1.874144e+05</td>\n",
       "      <td>-3.403640e+00</td>\n",
       "      <td>0.363671</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>80.569768</td>\n",
       "      <td>8.561525</td>\n",
       "      <td>...</td>\n",
       "      <td>541.321099</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.078478</td>\n",
       "      <td>-0.714148</td>\n",
       "      <td>0.240794</td>\n",
       "      <td>12.504444</td>\n",
       "      <td>0.794166</td>\n",
       "      <td>0.235056</td>\n",
       "      <td>0.997111</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802470</td>\n",
       "      <td>-0.135336</td>\n",
       "      <td>8.386524e+10</td>\n",
       "      <td>1.149721e+07</td>\n",
       "      <td>-3.401543e-02</td>\n",
       "      <td>0.909037</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>279.030686</td>\n",
       "      <td>21.257351</td>\n",
       "      <td>...</td>\n",
       "      <td>3237.989744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802470</td>\n",
       "      <td>-0.135336</td>\n",
       "      <td>0.816744</td>\n",
       "      <td>359.257296</td>\n",
       "      <td>26.876461</td>\n",
       "      <td>22.075470</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2             3             4   \\\n",
       "count  180.000000  180.000000  180.000000  1.800000e+02  1.800000e+02   \n",
       "mean     0.992072    0.091662   -0.784183  2.039197e+09  5.552481e+05   \n",
       "std      0.014959    0.173092    0.163485  1.092835e+10  1.872173e+06   \n",
       "min      0.915535    0.000663   -0.996293  6.122095e-02 -2.411734e+03   \n",
       "25%      0.993146    0.009910   -0.889751  6.712167e+00  3.004735e-02   \n",
       "50%      0.996141    0.021898   -0.808215  7.046733e+06  2.102329e+02   \n",
       "75%      0.998936    0.078478   -0.714148  2.342942e+08  1.874144e+05   \n",
       "max      1.000000    0.802470   -0.135336  8.386524e+10  1.149721e+07   \n",
       "\n",
       "                 5           6           7           8           9   \\\n",
       "count  1.800000e+02  180.000000  180.000000  180.000000  180.000000   \n",
       "mean  -4.183488e+08    0.307231   20.433333   67.791047    5.315244   \n",
       "std    2.286736e+09    0.198924   19.677313   52.806781    4.324071   \n",
       "min   -1.765460e+10    0.034849    0.000000   10.362369   -0.156562   \n",
       "25%   -6.626037e+07    0.176287    7.000000   34.342161    1.674326   \n",
       "50%   -1.388524e+06    0.246706   18.000000   49.015076    4.068749   \n",
       "75%   -3.403640e+00    0.363671   26.000000   80.569768    8.561525   \n",
       "max   -3.401543e-02    0.909037  108.000000  279.030686   21.257351   \n",
       "\n",
       "          ...               29          30          31          32  \\\n",
       "count     ...       180.000000  180.000000  180.000000  180.000000   \n",
       "mean      ...       408.621286    0.992072    0.091662   -0.784183   \n",
       "std       ...       699.357013    0.014959    0.173092    0.163485   \n",
       "min       ...         0.041068    0.915535    0.000663   -0.996293   \n",
       "25%       ...         0.758121    0.993146    0.009910   -0.889751   \n",
       "50%       ...        52.980775    0.996141    0.021898   -0.808215   \n",
       "75%       ...       541.321099    0.998936    0.078478   -0.714148   \n",
       "max       ...      3237.989744    1.000000    0.802470   -0.135336   \n",
       "\n",
       "               33          34          35           36          37          38  \n",
       "count  180.000000  180.000000  180.000000   180.000000  180.000000  180.000000  \n",
       "mean     0.218465    9.269826  -12.284393   -44.985478    0.923358    2.516667  \n",
       "std      0.164280   58.894120   85.276180   289.088442    0.098877    0.887373  \n",
       "min      0.033770 -254.020889 -658.725753 -2243.745500    0.629249    2.000000  \n",
       "25%      0.130432    0.518238    0.193711     0.066518    0.909347    2.000000  \n",
       "50%      0.162345    5.833418    0.497587     0.153729    0.957392    2.000000  \n",
       "75%      0.240794   12.504444    0.794166     0.235056    0.997111    3.000000  \n",
       "max      0.816744  359.257296   26.876461    22.075470    0.999723    6.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_norm = scaler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset = torch.FloatTensor(dataset_norm[:1])\n",
    "x_ranker  = torch.LongTensor(ranker[:1])\n",
    "x_target  = torch.FloatTensor(target[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.9510]), tensor([ 2.]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_dataset, x_ranker), x_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RankerDataset(dataset_norm[3:], ranker[3:], target[3:])\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankerNet(dataset_sz=39, ranker_sz=3, latent_sz=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32/180 (17%)]\tLoss: 68.413528\n",
      "Train Epoch: 1 [64/180 (33%)]\tLoss: 29.504915\n",
      "Train Epoch: 1 [96/180 (50%)]\tLoss: 21.400311\n",
      "Train Epoch: 1 [128/180 (67%)]\tLoss: 21.291174\n",
      "Train Epoch: 1 [100/180 (83%)]\tLoss: 8.021564\n",
      "\n",
      "Train Epoch: 2 [32/180 (17%)]\tLoss: 31.696821\n",
      "Train Epoch: 2 [64/180 (33%)]\tLoss: 8.570896\n",
      "Train Epoch: 2 [96/180 (50%)]\tLoss: 10.381214\n",
      "Train Epoch: 2 [128/180 (67%)]\tLoss: 12.946533\n",
      "Train Epoch: 2 [100/180 (83%)]\tLoss: 8.478603\n",
      "\n",
      "Train Epoch: 3 [32/180 (17%)]\tLoss: 15.714861\n",
      "Train Epoch: 3 [64/180 (33%)]\tLoss: 6.809967\n",
      "Train Epoch: 3 [96/180 (50%)]\tLoss: 4.818727\n",
      "Train Epoch: 3 [128/180 (67%)]\tLoss: 9.026245\n",
      "Train Epoch: 3 [100/180 (83%)]\tLoss: 9.277956\n",
      "\n",
      "Train Epoch: 4 [32/180 (17%)]\tLoss: 12.549003\n",
      "Train Epoch: 4 [64/180 (33%)]\tLoss: 6.202896\n",
      "Train Epoch: 4 [96/180 (50%)]\tLoss: 4.648726\n",
      "Train Epoch: 4 [128/180 (67%)]\tLoss: 5.537826\n",
      "Train Epoch: 4 [100/180 (83%)]\tLoss: 4.912756\n",
      "\n",
      "Train Epoch: 5 [32/180 (17%)]\tLoss: 7.980180\n",
      "Train Epoch: 5 [64/180 (33%)]\tLoss: 4.073342\n",
      "Train Epoch: 5 [96/180 (50%)]\tLoss: 2.438981\n",
      "Train Epoch: 5 [128/180 (67%)]\tLoss: 3.776118\n",
      "Train Epoch: 5 [100/180 (83%)]\tLoss: 6.458869\n",
      "\n",
      "Train Epoch: 6 [32/180 (17%)]\tLoss: 6.448734\n",
      "Train Epoch: 6 [64/180 (33%)]\tLoss: 2.415333\n",
      "Train Epoch: 6 [96/180 (50%)]\tLoss: 2.366513\n",
      "Train Epoch: 6 [128/180 (67%)]\tLoss: 3.876529\n",
      "Train Epoch: 6 [100/180 (83%)]\tLoss: 4.095338\n",
      "\n",
      "Train Epoch: 7 [32/180 (17%)]\tLoss: 4.473072\n",
      "Train Epoch: 7 [64/180 (33%)]\tLoss: 3.302233\n",
      "Train Epoch: 7 [96/180 (50%)]\tLoss: 2.363850\n",
      "Train Epoch: 7 [128/180 (67%)]\tLoss: 2.092492\n",
      "Train Epoch: 7 [100/180 (83%)]\tLoss: 2.731117\n",
      "\n",
      "Train Epoch: 8 [32/180 (17%)]\tLoss: 4.522382\n",
      "Train Epoch: 8 [64/180 (33%)]\tLoss: 1.718347\n",
      "Train Epoch: 8 [96/180 (50%)]\tLoss: 1.973699\n",
      "Train Epoch: 8 [128/180 (67%)]\tLoss: 2.165592\n",
      "Train Epoch: 8 [100/180 (83%)]\tLoss: 1.688005\n",
      "\n",
      "Train Epoch: 9 [32/180 (17%)]\tLoss: 2.962488\n",
      "Train Epoch: 9 [64/180 (33%)]\tLoss: 1.852425\n",
      "Train Epoch: 9 [96/180 (50%)]\tLoss: 1.747401\n",
      "Train Epoch: 9 [128/180 (67%)]\tLoss: 1.493938\n",
      "Train Epoch: 9 [100/180 (83%)]\tLoss: 1.950271\n",
      "\n",
      "Train Epoch: 10 [32/180 (17%)]\tLoss: 2.689381\n",
      "Train Epoch: 10 [64/180 (33%)]\tLoss: 2.186622\n",
      "Train Epoch: 10 [96/180 (50%)]\tLoss: 1.248873\n",
      "Train Epoch: 10 [128/180 (67%)]\tLoss: 1.161547\n",
      "Train Epoch: 10 [100/180 (83%)]\tLoss: 1.345478\n",
      "\n",
      "Train Epoch: 11 [32/180 (17%)]\tLoss: 2.174093\n",
      "Train Epoch: 11 [64/180 (33%)]\tLoss: 0.911244\n",
      "Train Epoch: 11 [96/180 (50%)]\tLoss: 1.417920\n",
      "Train Epoch: 11 [128/180 (67%)]\tLoss: 1.489924\n",
      "Train Epoch: 11 [100/180 (83%)]\tLoss: 1.755755\n",
      "\n",
      "Train Epoch: 12 [32/180 (17%)]\tLoss: 2.460153\n",
      "Train Epoch: 12 [64/180 (33%)]\tLoss: 1.001562\n",
      "Train Epoch: 12 [96/180 (50%)]\tLoss: 0.940463\n",
      "Train Epoch: 12 [128/180 (67%)]\tLoss: 1.439965\n",
      "Train Epoch: 12 [100/180 (83%)]\tLoss: 1.156132\n",
      "\n",
      "Train Epoch: 13 [32/180 (17%)]\tLoss: 2.022587\n",
      "Train Epoch: 13 [64/180 (33%)]\tLoss: 1.256249\n",
      "Train Epoch: 13 [96/180 (50%)]\tLoss: 0.730190\n",
      "Train Epoch: 13 [128/180 (67%)]\tLoss: 1.043404\n",
      "Train Epoch: 13 [100/180 (83%)]\tLoss: 1.629381\n",
      "\n",
      "Train Epoch: 14 [32/180 (17%)]\tLoss: 1.922426\n",
      "Train Epoch: 14 [64/180 (33%)]\tLoss: 0.946057\n",
      "Train Epoch: 14 [96/180 (50%)]\tLoss: 1.253749\n",
      "Train Epoch: 14 [128/180 (67%)]\tLoss: 0.968926\n",
      "Train Epoch: 14 [100/180 (83%)]\tLoss: 0.804311\n",
      "\n",
      "Train Epoch: 15 [32/180 (17%)]\tLoss: 1.738209\n",
      "Train Epoch: 15 [64/180 (33%)]\tLoss: 1.057302\n",
      "Train Epoch: 15 [96/180 (50%)]\tLoss: 0.665266\n",
      "Train Epoch: 15 [128/180 (67%)]\tLoss: 0.901462\n",
      "Train Epoch: 15 [100/180 (83%)]\tLoss: 1.434905\n",
      "\n",
      "Train Epoch: 16 [32/180 (17%)]\tLoss: 1.433966\n",
      "Train Epoch: 16 [64/180 (33%)]\tLoss: 1.183547\n",
      "Train Epoch: 16 [96/180 (50%)]\tLoss: 0.766259\n",
      "Train Epoch: 16 [128/180 (67%)]\tLoss: 0.745698\n",
      "Train Epoch: 16 [100/180 (83%)]\tLoss: 1.463667\n",
      "\n",
      "Train Epoch: 17 [32/180 (17%)]\tLoss: 1.688954\n",
      "Train Epoch: 17 [64/180 (33%)]\tLoss: 0.763444\n",
      "Train Epoch: 17 [96/180 (50%)]\tLoss: 0.928237\n",
      "Train Epoch: 17 [128/180 (67%)]\tLoss: 1.140380\n",
      "Train Epoch: 17 [100/180 (83%)]\tLoss: 0.415778\n",
      "\n",
      "Train Epoch: 18 [32/180 (17%)]\tLoss: 1.428670\n",
      "Train Epoch: 18 [64/180 (33%)]\tLoss: 0.797550\n",
      "Train Epoch: 18 [96/180 (50%)]\tLoss: 0.903313\n",
      "Train Epoch: 18 [128/180 (67%)]\tLoss: 0.810849\n",
      "Train Epoch: 18 [100/180 (83%)]\tLoss: 1.126460\n",
      "\n",
      "Train Epoch: 19 [32/180 (17%)]\tLoss: 1.438108\n",
      "Train Epoch: 19 [64/180 (33%)]\tLoss: 0.732344\n",
      "Train Epoch: 19 [96/180 (50%)]\tLoss: 0.760747\n",
      "Train Epoch: 19 [128/180 (67%)]\tLoss: 1.188788\n",
      "Train Epoch: 19 [100/180 (83%)]\tLoss: 0.544718\n",
      "\n",
      "Train Epoch: 20 [32/180 (17%)]\tLoss: 1.432220\n",
      "Train Epoch: 20 [64/180 (33%)]\tLoss: 0.975174\n",
      "Train Epoch: 20 [96/180 (50%)]\tLoss: 0.702219\n",
      "Train Epoch: 20 [128/180 (67%)]\tLoss: 0.812004\n",
      "Train Epoch: 20 [100/180 (83%)]\tLoss: 0.796658\n",
      "\n",
      "Train Epoch: 21 [32/180 (17%)]\tLoss: 1.271834\n",
      "Train Epoch: 21 [64/180 (33%)]\tLoss: 0.650523\n",
      "Train Epoch: 21 [96/180 (50%)]\tLoss: 0.920570\n",
      "Train Epoch: 21 [128/180 (67%)]\tLoss: 0.954694\n",
      "Train Epoch: 21 [100/180 (83%)]\tLoss: 0.609401\n",
      "\n",
      "Train Epoch: 22 [32/180 (17%)]\tLoss: 1.472388\n",
      "Train Epoch: 22 [64/180 (33%)]\tLoss: 0.877450\n",
      "Train Epoch: 22 [96/180 (50%)]\tLoss: 0.740656\n",
      "Train Epoch: 22 [128/180 (67%)]\tLoss: 0.831248\n",
      "Train Epoch: 22 [100/180 (83%)]\tLoss: 0.465582\n",
      "\n",
      "Train Epoch: 23 [32/180 (17%)]\tLoss: 1.141153\n",
      "Train Epoch: 23 [64/180 (33%)]\tLoss: 0.597415\n",
      "Train Epoch: 23 [96/180 (50%)]\tLoss: 0.929656\n",
      "Train Epoch: 23 [128/180 (67%)]\tLoss: 0.974002\n",
      "Train Epoch: 23 [100/180 (83%)]\tLoss: 0.611365\n",
      "\n",
      "Train Epoch: 24 [32/180 (17%)]\tLoss: 1.511217\n",
      "Train Epoch: 24 [64/180 (33%)]\tLoss: 0.634600\n",
      "Train Epoch: 24 [96/180 (50%)]\tLoss: 0.781128\n",
      "Train Epoch: 24 [128/180 (67%)]\tLoss: 0.638767\n",
      "Train Epoch: 24 [100/180 (83%)]\tLoss: 0.587456\n",
      "\n",
      "Train Epoch: 25 [32/180 (17%)]\tLoss: 1.103131\n",
      "Train Epoch: 25 [64/180 (33%)]\tLoss: 0.793362\n",
      "Train Epoch: 25 [96/180 (50%)]\tLoss: 0.719210\n",
      "Train Epoch: 25 [128/180 (67%)]\tLoss: 0.910486\n",
      "Train Epoch: 25 [100/180 (83%)]\tLoss: 0.547742\n",
      "\n",
      "Train Epoch: 26 [32/180 (17%)]\tLoss: 1.315205\n",
      "Train Epoch: 26 [64/180 (33%)]\tLoss: 0.718879\n",
      "Train Epoch: 26 [96/180 (50%)]\tLoss: 0.551454\n",
      "Train Epoch: 26 [128/180 (67%)]\tLoss: 0.802933\n",
      "Train Epoch: 26 [100/180 (83%)]\tLoss: 0.650868\n",
      "\n",
      "Train Epoch: 27 [32/180 (17%)]\tLoss: 1.545451\n",
      "Train Epoch: 27 [64/180 (33%)]\tLoss: 0.548766\n",
      "Train Epoch: 27 [96/180 (50%)]\tLoss: 0.818587\n",
      "Train Epoch: 27 [128/180 (67%)]\tLoss: 0.535417\n",
      "Train Epoch: 27 [100/180 (83%)]\tLoss: 0.612253\n",
      "\n",
      "Train Epoch: 28 [32/180 (17%)]\tLoss: 1.174037\n",
      "Train Epoch: 28 [64/180 (33%)]\tLoss: 0.763897\n",
      "Train Epoch: 28 [96/180 (50%)]\tLoss: 0.552311\n",
      "Train Epoch: 28 [128/180 (67%)]\tLoss: 0.487281\n",
      "Train Epoch: 28 [100/180 (83%)]\tLoss: 1.166676\n",
      "\n",
      "Train Epoch: 29 [32/180 (17%)]\tLoss: 0.871991\n",
      "Train Epoch: 29 [64/180 (33%)]\tLoss: 0.829931\n",
      "Train Epoch: 29 [96/180 (50%)]\tLoss: 0.669436\n",
      "Train Epoch: 29 [128/180 (67%)]\tLoss: 0.708654\n",
      "Train Epoch: 29 [100/180 (83%)]\tLoss: 0.764178\n",
      "\n",
      "Train Epoch: 30 [32/180 (17%)]\tLoss: 1.063457\n",
      "Train Epoch: 30 [64/180 (33%)]\tLoss: 0.657907\n",
      "Train Epoch: 30 [96/180 (50%)]\tLoss: 0.605593\n",
      "Train Epoch: 30 [128/180 (67%)]\tLoss: 0.715199\n",
      "Train Epoch: 30 [100/180 (83%)]\tLoss: 0.813197\n",
      "\n",
      "Train Epoch: 31 [32/180 (17%)]\tLoss: 1.175706\n",
      "Train Epoch: 31 [64/180 (33%)]\tLoss: 0.515820\n",
      "Train Epoch: 31 [96/180 (50%)]\tLoss: 0.682230\n",
      "Train Epoch: 31 [128/180 (67%)]\tLoss: 0.434223\n",
      "Train Epoch: 31 [100/180 (83%)]\tLoss: 1.191131\n",
      "\n",
      "Train Epoch: 32 [32/180 (17%)]\tLoss: 1.019921\n",
      "Train Epoch: 32 [64/180 (33%)]\tLoss: 0.698443\n",
      "Train Epoch: 32 [96/180 (50%)]\tLoss: 0.854938\n",
      "Train Epoch: 32 [128/180 (67%)]\tLoss: 0.650333\n",
      "Train Epoch: 32 [100/180 (83%)]\tLoss: 0.708437\n",
      "\n",
      "Train Epoch: 33 [32/180 (17%)]\tLoss: 1.002567\n",
      "Train Epoch: 33 [64/180 (33%)]\tLoss: 0.669760\n",
      "Train Epoch: 33 [96/180 (50%)]\tLoss: 0.591301\n",
      "Train Epoch: 33 [128/180 (67%)]\tLoss: 0.747183\n",
      "Train Epoch: 33 [100/180 (83%)]\tLoss: 0.724259\n",
      "\n",
      "Train Epoch: 34 [32/180 (17%)]\tLoss: 1.089768\n",
      "Train Epoch: 34 [64/180 (33%)]\tLoss: 0.599922\n",
      "Train Epoch: 34 [96/180 (50%)]\tLoss: 0.829946\n",
      "Train Epoch: 34 [128/180 (67%)]\tLoss: 0.517722\n",
      "Train Epoch: 34 [100/180 (83%)]\tLoss: 0.750727\n",
      "\n",
      "Train Epoch: 35 [32/180 (17%)]\tLoss: 1.203696\n",
      "Train Epoch: 35 [64/180 (33%)]\tLoss: 0.600133\n",
      "Train Epoch: 35 [96/180 (50%)]\tLoss: 0.614567\n",
      "Train Epoch: 35 [128/180 (67%)]\tLoss: 0.387562\n",
      "Train Epoch: 35 [100/180 (83%)]\tLoss: 0.907611\n",
      "\n",
      "Train Epoch: 36 [32/180 (17%)]\tLoss: 1.107035\n",
      "Train Epoch: 36 [64/180 (33%)]\tLoss: 0.759086\n",
      "Train Epoch: 36 [96/180 (50%)]\tLoss: 0.752334\n",
      "Train Epoch: 36 [128/180 (67%)]\tLoss: 0.415141\n",
      "Train Epoch: 36 [100/180 (83%)]\tLoss: 0.580059\n",
      "\n",
      "Train Epoch: 37 [32/180 (17%)]\tLoss: 1.243120\n",
      "Train Epoch: 37 [64/180 (33%)]\tLoss: 0.479703\n",
      "Train Epoch: 37 [96/180 (50%)]\tLoss: 0.684527\n",
      "Train Epoch: 37 [128/180 (67%)]\tLoss: 0.471183\n",
      "Train Epoch: 37 [100/180 (83%)]\tLoss: 0.825748\n",
      "\n",
      "Train Epoch: 38 [32/180 (17%)]\tLoss: 1.260365\n",
      "Train Epoch: 38 [64/180 (33%)]\tLoss: 0.522211\n",
      "Train Epoch: 38 [96/180 (50%)]\tLoss: 0.581602\n",
      "Train Epoch: 38 [128/180 (67%)]\tLoss: 0.641855\n",
      "Train Epoch: 38 [100/180 (83%)]\tLoss: 0.612018\n",
      "\n",
      "Train Epoch: 39 [32/180 (17%)]\tLoss: 1.266343\n",
      "Train Epoch: 39 [64/180 (33%)]\tLoss: 0.525638\n",
      "Train Epoch: 39 [96/180 (50%)]\tLoss: 0.630879\n",
      "Train Epoch: 39 [128/180 (67%)]\tLoss: 0.580824\n",
      "Train Epoch: 39 [100/180 (83%)]\tLoss: 0.507783\n",
      "\n",
      "Train Epoch: 40 [32/180 (17%)]\tLoss: 1.090613\n",
      "Train Epoch: 40 [64/180 (33%)]\tLoss: 0.860382\n",
      "Train Epoch: 40 [96/180 (50%)]\tLoss: 0.600457\n",
      "Train Epoch: 40 [128/180 (67%)]\tLoss: 0.422727\n",
      "Train Epoch: 40 [100/180 (83%)]\tLoss: 0.553704\n",
      "\n",
      "Train Epoch: 41 [32/180 (17%)]\tLoss: 1.182060\n",
      "Train Epoch: 41 [64/180 (33%)]\tLoss: 0.413194\n",
      "Train Epoch: 41 [96/180 (50%)]\tLoss: 0.498368\n",
      "Train Epoch: 41 [128/180 (67%)]\tLoss: 0.710805\n",
      "Train Epoch: 41 [100/180 (83%)]\tLoss: 0.717923\n",
      "\n",
      "Train Epoch: 42 [32/180 (17%)]\tLoss: 1.047171\n",
      "Train Epoch: 42 [64/180 (33%)]\tLoss: 0.590042\n",
      "Train Epoch: 42 [96/180 (50%)]\tLoss: 0.553050\n",
      "Train Epoch: 42 [128/180 (67%)]\tLoss: 0.729834\n",
      "Train Epoch: 42 [100/180 (83%)]\tLoss: 0.504440\n",
      "\n",
      "Train Epoch: 43 [32/180 (17%)]\tLoss: 1.234993\n",
      "Train Epoch: 43 [64/180 (33%)]\tLoss: 0.564474\n",
      "Train Epoch: 43 [96/180 (50%)]\tLoss: 0.447177\n",
      "Train Epoch: 43 [128/180 (67%)]\tLoss: 0.542702\n",
      "Train Epoch: 43 [100/180 (83%)]\tLoss: 0.804598\n",
      "\n",
      "Train Epoch: 44 [32/180 (17%)]\tLoss: 1.093120\n",
      "Train Epoch: 44 [64/180 (33%)]\tLoss: 0.418620\n",
      "Train Epoch: 44 [96/180 (50%)]\tLoss: 0.535755\n",
      "Train Epoch: 44 [128/180 (67%)]\tLoss: 0.936514\n",
      "Train Epoch: 44 [100/180 (83%)]\tLoss: 0.572504\n",
      "\n",
      "Train Epoch: 45 [32/180 (17%)]\tLoss: 1.003573\n",
      "Train Epoch: 45 [64/180 (33%)]\tLoss: 0.753629\n",
      "Train Epoch: 45 [96/180 (50%)]\tLoss: 0.582640\n",
      "Train Epoch: 45 [128/180 (67%)]\tLoss: 0.585476\n",
      "Train Epoch: 45 [100/180 (83%)]\tLoss: 0.497574\n",
      "\n",
      "Train Epoch: 46 [32/180 (17%)]\tLoss: 1.700342\n",
      "Train Epoch: 46 [64/180 (33%)]\tLoss: 0.364953\n",
      "Train Epoch: 46 [96/180 (50%)]\tLoss: 0.348728\n",
      "Train Epoch: 46 [128/180 (67%)]\tLoss: 0.700281\n",
      "Train Epoch: 46 [100/180 (83%)]\tLoss: 0.707114\n",
      "\n",
      "Train Epoch: 47 [32/180 (17%)]\tLoss: 1.034103\n",
      "Train Epoch: 47 [64/180 (33%)]\tLoss: 0.842340\n",
      "Train Epoch: 47 [96/180 (50%)]\tLoss: 0.575801\n",
      "Train Epoch: 47 [128/180 (67%)]\tLoss: 0.455606\n",
      "Train Epoch: 47 [100/180 (83%)]\tLoss: 0.427318\n",
      "\n",
      "Train Epoch: 48 [32/180 (17%)]\tLoss: 1.256143\n",
      "Train Epoch: 48 [64/180 (33%)]\tLoss: 0.507686\n",
      "Train Epoch: 48 [96/180 (50%)]\tLoss: 0.431927\n",
      "Train Epoch: 48 [128/180 (67%)]\tLoss: 0.509528\n",
      "Train Epoch: 48 [100/180 (83%)]\tLoss: 0.682332\n",
      "\n",
      "Train Epoch: 49 [32/180 (17%)]\tLoss: 0.961083\n",
      "Train Epoch: 49 [64/180 (33%)]\tLoss: 0.631538\n",
      "Train Epoch: 49 [96/180 (50%)]\tLoss: 0.684309\n",
      "Train Epoch: 49 [128/180 (67%)]\tLoss: 0.545918\n",
      "Train Epoch: 49 [100/180 (83%)]\tLoss: 0.562029\n",
      "\n",
      "Train Epoch: 50 [32/180 (17%)]\tLoss: 0.939214\n",
      "Train Epoch: 50 [64/180 (33%)]\tLoss: 0.677620\n",
      "Train Epoch: 50 [96/180 (50%)]\tLoss: 0.620469\n",
      "Train Epoch: 50 [128/180 (67%)]\tLoss: 0.461244\n",
      "Train Epoch: 50 [100/180 (83%)]\tLoss: 0.578419\n",
      "\n",
      "Train Epoch: 51 [32/180 (17%)]\tLoss: 0.992242\n",
      "Train Epoch: 51 [64/180 (33%)]\tLoss: 0.476356\n",
      "Train Epoch: 51 [96/180 (50%)]\tLoss: 0.551414\n",
      "Train Epoch: 51 [128/180 (67%)]\tLoss: 0.777625\n",
      "Train Epoch: 51 [100/180 (83%)]\tLoss: 0.441365\n",
      "\n",
      "Train Epoch: 52 [32/180 (17%)]\tLoss: 1.028534\n",
      "Train Epoch: 52 [64/180 (33%)]\tLoss: 0.371614\n",
      "Train Epoch: 52 [96/180 (50%)]\tLoss: 0.572828\n",
      "Train Epoch: 52 [128/180 (67%)]\tLoss: 0.866670\n",
      "Train Epoch: 52 [100/180 (83%)]\tLoss: 0.518423\n",
      "\n",
      "Train Epoch: 53 [32/180 (17%)]\tLoss: 0.951506\n",
      "Train Epoch: 53 [64/180 (33%)]\tLoss: 0.719897\n",
      "Train Epoch: 53 [96/180 (50%)]\tLoss: 0.476962\n",
      "Train Epoch: 53 [128/180 (67%)]\tLoss: 0.474809\n",
      "Train Epoch: 53 [100/180 (83%)]\tLoss: 0.738154\n",
      "\n",
      "Train Epoch: 54 [32/180 (17%)]\tLoss: 1.008925\n",
      "Train Epoch: 54 [64/180 (33%)]\tLoss: 0.537419\n",
      "Train Epoch: 54 [96/180 (50%)]\tLoss: 0.513829\n",
      "Train Epoch: 54 [128/180 (67%)]\tLoss: 0.638580\n",
      "Train Epoch: 54 [100/180 (83%)]\tLoss: 0.664709\n",
      "\n",
      "Train Epoch: 55 [32/180 (17%)]\tLoss: 1.127638\n",
      "Train Epoch: 55 [64/180 (33%)]\tLoss: 0.425792\n",
      "Train Epoch: 55 [96/180 (50%)]\tLoss: 0.462528\n",
      "Train Epoch: 55 [128/180 (67%)]\tLoss: 0.690837\n",
      "Train Epoch: 55 [100/180 (83%)]\tLoss: 0.410382\n",
      "\n",
      "Train Epoch: 56 [32/180 (17%)]\tLoss: 1.020947\n",
      "Train Epoch: 56 [64/180 (33%)]\tLoss: 0.371803\n",
      "Train Epoch: 56 [96/180 (50%)]\tLoss: 0.736786\n",
      "Train Epoch: 56 [128/180 (67%)]\tLoss: 0.725550\n",
      "Train Epoch: 56 [100/180 (83%)]\tLoss: 0.321121\n",
      "\n",
      "Train Epoch: 57 [32/180 (17%)]\tLoss: 0.875414\n",
      "Train Epoch: 57 [64/180 (33%)]\tLoss: 0.576881\n",
      "Train Epoch: 57 [96/180 (50%)]\tLoss: 0.808548\n",
      "Train Epoch: 57 [128/180 (67%)]\tLoss: 0.483475\n",
      "Train Epoch: 57 [100/180 (83%)]\tLoss: 0.598865\n",
      "\n",
      "Train Epoch: 58 [32/180 (17%)]\tLoss: 0.864297\n",
      "Train Epoch: 58 [64/180 (33%)]\tLoss: 0.528848\n",
      "Train Epoch: 58 [96/180 (50%)]\tLoss: 0.656436\n",
      "Train Epoch: 58 [128/180 (67%)]\tLoss: 0.566930\n",
      "Train Epoch: 58 [100/180 (83%)]\tLoss: 0.822812\n",
      "\n",
      "Train Epoch: 59 [32/180 (17%)]\tLoss: 1.002107\n",
      "Train Epoch: 59 [64/180 (33%)]\tLoss: 0.497120\n",
      "Train Epoch: 59 [96/180 (50%)]\tLoss: 0.590343\n",
      "Train Epoch: 59 [128/180 (67%)]\tLoss: 0.542716\n",
      "Train Epoch: 59 [100/180 (83%)]\tLoss: 0.546716\n",
      "\n",
      "Train Epoch: 60 [32/180 (17%)]\tLoss: 1.089794\n",
      "Train Epoch: 60 [64/180 (33%)]\tLoss: 0.632715\n",
      "Train Epoch: 60 [96/180 (50%)]\tLoss: 0.515935\n",
      "Train Epoch: 60 [128/180 (67%)]\tLoss: 0.643870\n",
      "Train Epoch: 60 [100/180 (83%)]\tLoss: 0.954656\n",
      "\n",
      "Train Epoch: 61 [32/180 (17%)]\tLoss: 0.959741\n",
      "Train Epoch: 61 [64/180 (33%)]\tLoss: 0.398880\n",
      "Train Epoch: 61 [96/180 (50%)]\tLoss: 1.021221\n",
      "Train Epoch: 61 [128/180 (67%)]\tLoss: 0.860657\n",
      "Train Epoch: 61 [100/180 (83%)]\tLoss: 0.860047\n",
      "\n",
      "Train Epoch: 62 [32/180 (17%)]\tLoss: 1.123903\n",
      "Train Epoch: 62 [64/180 (33%)]\tLoss: 0.755705\n",
      "Train Epoch: 62 [96/180 (50%)]\tLoss: 0.517234\n",
      "Train Epoch: 62 [128/180 (67%)]\tLoss: 1.151432\n",
      "Train Epoch: 62 [100/180 (83%)]\tLoss: 0.667272\n",
      "\n",
      "Train Epoch: 63 [32/180 (17%)]\tLoss: 1.513381\n",
      "Train Epoch: 63 [64/180 (33%)]\tLoss: 0.356568\n",
      "Train Epoch: 63 [96/180 (50%)]\tLoss: 0.553968\n",
      "Train Epoch: 63 [128/180 (67%)]\tLoss: 0.755104\n",
      "Train Epoch: 63 [100/180 (83%)]\tLoss: 0.341104\n",
      "\n",
      "Train Epoch: 64 [32/180 (17%)]\tLoss: 3.038541\n",
      "Train Epoch: 64 [64/180 (33%)]\tLoss: 0.603886\n",
      "Train Epoch: 64 [96/180 (50%)]\tLoss: 0.924102\n",
      "Train Epoch: 64 [128/180 (67%)]\tLoss: 0.553766\n",
      "Train Epoch: 64 [100/180 (83%)]\tLoss: 0.577363\n",
      "\n",
      "Train Epoch: 65 [32/180 (17%)]\tLoss: 1.200978\n",
      "Train Epoch: 65 [64/180 (33%)]\tLoss: 1.066601\n",
      "Train Epoch: 65 [96/180 (50%)]\tLoss: 0.583902\n",
      "Train Epoch: 65 [128/180 (67%)]\tLoss: 1.193087\n",
      "Train Epoch: 65 [100/180 (83%)]\tLoss: 0.738960\n",
      "\n",
      "Train Epoch: 66 [32/180 (17%)]\tLoss: 1.226075\n",
      "Train Epoch: 66 [64/180 (33%)]\tLoss: 0.645629\n",
      "Train Epoch: 66 [96/180 (50%)]\tLoss: 0.429099\n",
      "Train Epoch: 66 [128/180 (67%)]\tLoss: 0.653149\n",
      "Train Epoch: 66 [100/180 (83%)]\tLoss: 0.312356\n",
      "\n",
      "Train Epoch: 67 [32/180 (17%)]\tLoss: 1.037386\n",
      "Train Epoch: 67 [64/180 (33%)]\tLoss: 0.545680\n",
      "Train Epoch: 67 [96/180 (50%)]\tLoss: 0.571442\n",
      "Train Epoch: 67 [128/180 (67%)]\tLoss: 0.540321\n",
      "Train Epoch: 67 [100/180 (83%)]\tLoss: 0.488011\n",
      "\n",
      "Train Epoch: 68 [32/180 (17%)]\tLoss: 1.101278\n",
      "Train Epoch: 68 [64/180 (33%)]\tLoss: 0.527305\n",
      "Train Epoch: 68 [96/180 (50%)]\tLoss: 0.486934\n",
      "Train Epoch: 68 [128/180 (67%)]\tLoss: 0.591535\n",
      "Train Epoch: 68 [100/180 (83%)]\tLoss: 0.577331\n",
      "\n",
      "Train Epoch: 69 [32/180 (17%)]\tLoss: 1.186374\n",
      "Train Epoch: 69 [64/180 (33%)]\tLoss: 0.539106\n",
      "Train Epoch: 69 [96/180 (50%)]\tLoss: 0.475964\n",
      "Train Epoch: 69 [128/180 (67%)]\tLoss: 0.564345\n",
      "Train Epoch: 69 [100/180 (83%)]\tLoss: 0.439657\n",
      "\n",
      "Train Epoch: 70 [32/180 (17%)]\tLoss: 1.088987\n",
      "Train Epoch: 70 [64/180 (33%)]\tLoss: 0.553761\n",
      "Train Epoch: 70 [96/180 (50%)]\tLoss: 0.619364\n",
      "Train Epoch: 70 [128/180 (67%)]\tLoss: 0.609122\n",
      "Train Epoch: 70 [100/180 (83%)]\tLoss: 0.292788\n",
      "\n",
      "Train Epoch: 71 [32/180 (17%)]\tLoss: 1.202876\n",
      "Train Epoch: 71 [64/180 (33%)]\tLoss: 0.470213\n",
      "Train Epoch: 71 [96/180 (50%)]\tLoss: 0.577142\n",
      "Train Epoch: 71 [128/180 (67%)]\tLoss: 0.409795\n",
      "Train Epoch: 71 [100/180 (83%)]\tLoss: 0.383520\n",
      "\n",
      "Train Epoch: 72 [32/180 (17%)]\tLoss: 1.192023\n",
      "Train Epoch: 72 [64/180 (33%)]\tLoss: 0.724252\n",
      "Train Epoch: 72 [96/180 (50%)]\tLoss: 0.723109\n",
      "Train Epoch: 72 [128/180 (67%)]\tLoss: 0.556349\n",
      "Train Epoch: 72 [100/180 (83%)]\tLoss: 0.463922\n",
      "\n",
      "Train Epoch: 73 [32/180 (17%)]\tLoss: 0.887165\n",
      "Train Epoch: 73 [64/180 (33%)]\tLoss: 0.920549\n",
      "Train Epoch: 73 [96/180 (50%)]\tLoss: 0.472305\n",
      "Train Epoch: 73 [128/180 (67%)]\tLoss: 0.636970\n",
      "Train Epoch: 73 [100/180 (83%)]\tLoss: 0.543084\n",
      "\n",
      "Train Epoch: 74 [32/180 (17%)]\tLoss: 0.988053\n",
      "Train Epoch: 74 [64/180 (33%)]\tLoss: 0.562705\n",
      "Train Epoch: 74 [96/180 (50%)]\tLoss: 0.478756\n",
      "Train Epoch: 74 [128/180 (67%)]\tLoss: 0.766091\n",
      "Train Epoch: 74 [100/180 (83%)]\tLoss: 0.735234\n",
      "\n",
      "Train Epoch: 75 [32/180 (17%)]\tLoss: 1.068194\n",
      "Train Epoch: 75 [64/180 (33%)]\tLoss: 0.459107\n",
      "Train Epoch: 75 [96/180 (50%)]\tLoss: 0.737244\n",
      "Train Epoch: 75 [128/180 (67%)]\tLoss: 0.280862\n",
      "Train Epoch: 75 [100/180 (83%)]\tLoss: 0.833246\n",
      "\n",
      "Train Epoch: 76 [32/180 (17%)]\tLoss: 1.109452\n",
      "Train Epoch: 76 [64/180 (33%)]\tLoss: 0.327800\n",
      "Train Epoch: 76 [96/180 (50%)]\tLoss: 0.606040\n",
      "Train Epoch: 76 [128/180 (67%)]\tLoss: 0.613089\n",
      "Train Epoch: 76 [100/180 (83%)]\tLoss: 0.360775\n",
      "\n",
      "Train Epoch: 77 [32/180 (17%)]\tLoss: 1.093785\n",
      "Train Epoch: 77 [64/180 (33%)]\tLoss: 0.457532\n",
      "Train Epoch: 77 [96/180 (50%)]\tLoss: 0.464973\n",
      "Train Epoch: 77 [128/180 (67%)]\tLoss: 0.589049\n",
      "Train Epoch: 77 [100/180 (83%)]\tLoss: 0.870443\n",
      "\n",
      "Train Epoch: 78 [32/180 (17%)]\tLoss: 1.105640\n",
      "Train Epoch: 78 [64/180 (33%)]\tLoss: 0.725304\n",
      "Train Epoch: 78 [96/180 (50%)]\tLoss: 0.436985\n",
      "Train Epoch: 78 [128/180 (67%)]\tLoss: 0.490328\n",
      "Train Epoch: 78 [100/180 (83%)]\tLoss: 0.468105\n",
      "\n",
      "Train Epoch: 79 [32/180 (17%)]\tLoss: 0.836676\n",
      "Train Epoch: 79 [64/180 (33%)]\tLoss: 0.599618\n",
      "Train Epoch: 79 [96/180 (50%)]\tLoss: 0.785785\n",
      "Train Epoch: 79 [128/180 (67%)]\tLoss: 0.438653\n",
      "Train Epoch: 79 [100/180 (83%)]\tLoss: 0.515072\n",
      "\n",
      "Train Epoch: 80 [32/180 (17%)]\tLoss: 1.015723\n",
      "Train Epoch: 80 [64/180 (33%)]\tLoss: 0.446250\n",
      "Train Epoch: 80 [96/180 (50%)]\tLoss: 0.520426\n",
      "Train Epoch: 80 [128/180 (67%)]\tLoss: 0.489745\n",
      "Train Epoch: 80 [100/180 (83%)]\tLoss: 0.475945\n",
      "\n",
      "Train Epoch: 81 [32/180 (17%)]\tLoss: 0.768704\n",
      "Train Epoch: 81 [64/180 (33%)]\tLoss: 0.570887\n",
      "Train Epoch: 81 [96/180 (50%)]\tLoss: 0.574788\n",
      "Train Epoch: 81 [128/180 (67%)]\tLoss: 0.438206\n",
      "Train Epoch: 81 [100/180 (83%)]\tLoss: 0.700141\n",
      "\n",
      "Train Epoch: 82 [32/180 (17%)]\tLoss: 0.993437\n",
      "Train Epoch: 82 [64/180 (33%)]\tLoss: 0.544464\n",
      "Train Epoch: 82 [96/180 (50%)]\tLoss: 0.463082\n",
      "Train Epoch: 82 [128/180 (67%)]\tLoss: 0.704377\n",
      "Train Epoch: 82 [100/180 (83%)]\tLoss: 0.264931\n",
      "\n",
      "Train Epoch: 83 [32/180 (17%)]\tLoss: 1.017803\n",
      "Train Epoch: 83 [64/180 (33%)]\tLoss: 0.446103\n",
      "Train Epoch: 83 [96/180 (50%)]\tLoss: 0.668730\n",
      "Train Epoch: 83 [128/180 (67%)]\tLoss: 0.441591\n",
      "Train Epoch: 83 [100/180 (83%)]\tLoss: 0.601558\n",
      "\n",
      "Train Epoch: 84 [32/180 (17%)]\tLoss: 0.789212\n",
      "Train Epoch: 84 [64/180 (33%)]\tLoss: 0.559952\n",
      "Train Epoch: 84 [96/180 (50%)]\tLoss: 0.283932\n",
      "Train Epoch: 84 [128/180 (67%)]\tLoss: 0.818590\n",
      "Train Epoch: 84 [100/180 (83%)]\tLoss: 0.632454\n",
      "\n",
      "Train Epoch: 85 [32/180 (17%)]\tLoss: 1.067482\n",
      "Train Epoch: 85 [64/180 (33%)]\tLoss: 0.508145\n",
      "Train Epoch: 85 [96/180 (50%)]\tLoss: 0.494902\n",
      "Train Epoch: 85 [128/180 (67%)]\tLoss: 0.356262\n",
      "Train Epoch: 85 [100/180 (83%)]\tLoss: 0.704493\n",
      "\n",
      "Train Epoch: 86 [32/180 (17%)]\tLoss: 0.926694\n",
      "Train Epoch: 86 [64/180 (33%)]\tLoss: 0.585296\n",
      "Train Epoch: 86 [96/180 (50%)]\tLoss: 0.358487\n",
      "Train Epoch: 86 [128/180 (67%)]\tLoss: 0.733944\n",
      "Train Epoch: 86 [100/180 (83%)]\tLoss: 0.662943\n",
      "\n",
      "Train Epoch: 87 [32/180 (17%)]\tLoss: 1.002272\n",
      "Train Epoch: 87 [64/180 (33%)]\tLoss: 0.685675\n",
      "Train Epoch: 87 [96/180 (50%)]\tLoss: 0.477099\n",
      "Train Epoch: 87 [128/180 (67%)]\tLoss: 0.701582\n",
      "Train Epoch: 87 [100/180 (83%)]\tLoss: 0.733112\n",
      "\n",
      "Train Epoch: 88 [32/180 (17%)]\tLoss: 1.102897\n",
      "Train Epoch: 88 [64/180 (33%)]\tLoss: 0.667080\n",
      "Train Epoch: 88 [96/180 (50%)]\tLoss: 0.588420\n",
      "Train Epoch: 88 [128/180 (67%)]\tLoss: 0.476270\n",
      "Train Epoch: 88 [100/180 (83%)]\tLoss: 0.836519\n",
      "\n",
      "Train Epoch: 89 [32/180 (17%)]\tLoss: 1.372513\n",
      "Train Epoch: 89 [64/180 (33%)]\tLoss: 0.422448\n",
      "Train Epoch: 89 [96/180 (50%)]\tLoss: 0.660485\n",
      "Train Epoch: 89 [128/180 (67%)]\tLoss: 0.552073\n",
      "Train Epoch: 89 [100/180 (83%)]\tLoss: 0.977291\n",
      "\n",
      "Train Epoch: 90 [32/180 (17%)]\tLoss: 1.078845\n",
      "Train Epoch: 90 [64/180 (33%)]\tLoss: 0.401091\n",
      "Train Epoch: 90 [96/180 (50%)]\tLoss: 0.467777\n",
      "Train Epoch: 90 [128/180 (67%)]\tLoss: 0.443682\n",
      "Train Epoch: 90 [100/180 (83%)]\tLoss: 0.569434\n",
      "\n",
      "Train Epoch: 91 [32/180 (17%)]\tLoss: 1.625406\n",
      "Train Epoch: 91 [64/180 (33%)]\tLoss: 0.612118\n",
      "Train Epoch: 91 [96/180 (50%)]\tLoss: 0.558028\n",
      "Train Epoch: 91 [128/180 (67%)]\tLoss: 0.521152\n",
      "Train Epoch: 91 [100/180 (83%)]\tLoss: 0.425811\n",
      "\n",
      "Train Epoch: 92 [32/180 (17%)]\tLoss: 1.441498\n",
      "Train Epoch: 92 [64/180 (33%)]\tLoss: 0.817830\n",
      "Train Epoch: 92 [96/180 (50%)]\tLoss: 0.533105\n",
      "Train Epoch: 92 [128/180 (67%)]\tLoss: 0.500407\n",
      "Train Epoch: 92 [100/180 (83%)]\tLoss: 0.677576\n",
      "\n",
      "Train Epoch: 93 [32/180 (17%)]\tLoss: 1.110847\n",
      "Train Epoch: 93 [64/180 (33%)]\tLoss: 0.887280\n",
      "Train Epoch: 93 [96/180 (50%)]\tLoss: 0.471187\n",
      "Train Epoch: 93 [128/180 (67%)]\tLoss: 0.441503\n",
      "Train Epoch: 93 [100/180 (83%)]\tLoss: 0.750713\n",
      "\n",
      "Train Epoch: 94 [32/180 (17%)]\tLoss: 1.738306\n",
      "Train Epoch: 94 [64/180 (33%)]\tLoss: 0.747355\n",
      "Train Epoch: 94 [96/180 (50%)]\tLoss: 0.549013\n",
      "Train Epoch: 94 [128/180 (67%)]\tLoss: 0.517880\n",
      "Train Epoch: 94 [100/180 (83%)]\tLoss: 0.913369\n",
      "\n",
      "Train Epoch: 95 [32/180 (17%)]\tLoss: 1.727556\n",
      "Train Epoch: 95 [64/180 (33%)]\tLoss: 0.549710\n",
      "Train Epoch: 95 [96/180 (50%)]\tLoss: 0.748789\n",
      "Train Epoch: 95 [128/180 (67%)]\tLoss: 0.579265\n",
      "Train Epoch: 95 [100/180 (83%)]\tLoss: 0.581400\n",
      "\n",
      "Train Epoch: 96 [32/180 (17%)]\tLoss: 2.037021\n",
      "Train Epoch: 96 [64/180 (33%)]\tLoss: 1.140668\n",
      "Train Epoch: 96 [96/180 (50%)]\tLoss: 0.369065\n",
      "Train Epoch: 96 [128/180 (67%)]\tLoss: 0.448158\n",
      "Train Epoch: 96 [100/180 (83%)]\tLoss: 0.933600\n",
      "\n",
      "Train Epoch: 97 [32/180 (17%)]\tLoss: 1.749575\n",
      "Train Epoch: 97 [64/180 (33%)]\tLoss: 0.589586\n",
      "Train Epoch: 97 [96/180 (50%)]\tLoss: 0.670337\n",
      "Train Epoch: 97 [128/180 (67%)]\tLoss: 2.062468\n",
      "Train Epoch: 97 [100/180 (83%)]\tLoss: 0.566067\n",
      "\n",
      "Train Epoch: 98 [32/180 (17%)]\tLoss: 1.782984\n",
      "Train Epoch: 98 [64/180 (33%)]\tLoss: 1.278325\n",
      "Train Epoch: 98 [96/180 (50%)]\tLoss: 0.579038\n",
      "Train Epoch: 98 [128/180 (67%)]\tLoss: 0.581263\n",
      "Train Epoch: 98 [100/180 (83%)]\tLoss: 0.574425\n",
      "\n",
      "Train Epoch: 99 [32/180 (17%)]\tLoss: 2.023165\n",
      "Train Epoch: 99 [64/180 (33%)]\tLoss: 0.665894\n",
      "Train Epoch: 99 [96/180 (50%)]\tLoss: 0.728645\n",
      "Train Epoch: 99 [128/180 (67%)]\tLoss: 1.125345\n",
      "Train Epoch: 99 [100/180 (83%)]\tLoss: 0.531944\n",
      "\n",
      "Train Epoch: 100 [32/180 (17%)]\tLoss: 1.409517\n",
      "Train Epoch: 100 [64/180 (33%)]\tLoss: 1.861222\n",
      "Train Epoch: 100 [96/180 (50%)]\tLoss: 0.909185\n",
      "Train Epoch: 100 [128/180 (67%)]\tLoss: 0.424459\n",
      "Train Epoch: 100 [100/180 (83%)]\tLoss: 0.463809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, train_losses = train_model(model, dl, optimizer, criterion, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lgbm.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4121804313016847"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(target, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
